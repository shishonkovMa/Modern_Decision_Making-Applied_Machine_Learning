{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice assignment: Advanced ensembling techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XgQKq6Ldl-g"
   },
   "source": [
    "In this programming assignment, you are going to work with a dataset based on the following data:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n",
    "\n",
    "_Citation:_\n",
    "\n",
    "* _K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal._\n",
    "\n",
    "The dataset contains the information about the internet news articles. In this assignment, you are going to predict a number of shares of the news article (target column: `shares`). The information about the features is available through the link above. You are going to construct several machine learning algorithms (XGBoost, LightGBM, CatBoost and Lasso) and blend them into the final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qx7Y3W-_3LPT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ROnt-v3A5Rzy"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-95PM9m56JZ"
   },
   "source": [
    "## 1\n",
    "\n",
    "**q1:** How many missing values are there in the data? Provide the number of cells in the dataframe that contain NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AuyhBV3ULyIj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.917808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.407072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572592</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.496875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.396465</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.850694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.352778</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.583732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.804762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1578.0</td>\n",
       "      <td>0.429864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624595</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.593790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.332037</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>10.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.942282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.360069</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>11.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795082</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.702439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>12.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652422</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.636029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.376667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>13.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.273927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.433333</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>9.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.578275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748538</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.504702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.112500</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0                 6.0              73.0         0.916667               1.0   \n",
       "1                10.0            1280.0         0.407072               1.0   \n",
       "2                 9.0             576.0         0.535088               1.0   \n",
       "3                 9.0             210.0         0.583732               1.0   \n",
       "4                13.0            1578.0         0.429864               1.0   \n",
       "...               ...               ...              ...               ...   \n",
       "39639            10.0             745.0         0.487500               1.0   \n",
       "39640            11.0             205.0         0.671642               1.0   \n",
       "39641            12.0             544.0         0.551102               1.0   \n",
       "39642            13.0             303.0         0.574468               1.0   \n",
       "39643             9.0             319.0         0.578275               1.0   \n",
       "\n",
       "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  \\\n",
       "0                      0.980392        6.0             0.0       1.0   \n",
       "1                      0.572592       53.0             2.0      10.0   \n",
       "2                      0.693182        9.0             5.0       1.0   \n",
       "3                      0.729927        6.0             2.0       1.0   \n",
       "4                      0.624595       34.0            24.0      11.0   \n",
       "...                         ...        ...             ...       ...   \n",
       "39639                  0.587629       29.0             3.0       1.0   \n",
       "39640                  0.795082        5.0             1.0       0.0   \n",
       "39641                  0.652422       27.0            10.0      12.0   \n",
       "39642                  0.740113        3.0             3.0       3.0   \n",
       "39643                  0.748538       10.0             6.0       0.0   \n",
       "\n",
       "       num_videos  average_token_length  ...  min_positive_polarity  \\\n",
       "0             0.0              4.917808  ...               0.150000   \n",
       "1             1.0              4.496875  ...               0.033333   \n",
       "2             1.0              4.850694  ...               0.100000   \n",
       "3             0.0              4.804762  ...               0.062500   \n",
       "4             0.0              4.593790  ...               0.033333   \n",
       "...           ...                   ...  ...                    ...   \n",
       "39639        21.0              4.942282  ...               0.100000   \n",
       "39640         1.0              4.702439  ...               0.100000   \n",
       "39641         1.0              4.636029  ...               0.033333   \n",
       "39642         0.0              4.273927  ...               0.100000   \n",
       "39643         0.0              4.504702  ...               0.100000   \n",
       "\n",
       "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                   0.500000              -0.200000                 -0.200   \n",
       "1                   1.000000              -0.396465                 -1.000   \n",
       "2                   0.900000              -0.352778                 -1.000   \n",
       "3                   0.500000              -0.250000                 -0.400   \n",
       "4                   1.000000              -0.332037                 -1.000   \n",
       "...                      ...                    ...                    ...   \n",
       "39639               0.800000              -0.360069                 -0.800   \n",
       "39640               0.433333              -0.466667                 -0.800   \n",
       "39641               0.600000              -0.376667                 -0.800   \n",
       "39642               1.000000              -0.433333                 -0.500   \n",
       "39643               0.900000              -0.112500                 -0.125   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "0                  -0.200000            0.000000                  0.000000   \n",
       "1                  -0.050000            0.000000                  0.000000   \n",
       "2                  -0.100000            0.000000                  0.000000   \n",
       "3                  -0.100000            0.454545                  0.136364   \n",
       "4                  -0.071429            0.000000                  0.000000   \n",
       "...                      ...                 ...                       ...   \n",
       "39639              -0.066667            0.900000                 -0.500000   \n",
       "39640              -0.200000            0.600000                  0.400000   \n",
       "39641              -0.050000            0.666667                 -0.125000   \n",
       "39642              -0.300000            0.000000                  0.000000   \n",
       "39643              -0.100000            0.200000                 -0.100000   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity  shares  \n",
       "0                    0.500000                      0.000000     878  \n",
       "1                    0.500000                      0.000000    1100  \n",
       "2                    0.500000                      0.000000     872  \n",
       "3                    0.045455                      0.136364    5000  \n",
       "4                    0.500000                      0.000000    1600  \n",
       "...                       ...                           ...     ...  \n",
       "39639                0.400000                      0.500000    5800  \n",
       "39640                0.100000                      0.400000    1100  \n",
       "39641                0.166667                      0.125000     845  \n",
       "39642                0.500000                      0.000000    1300  \n",
       "39643                0.300000                      0.100000    1800  \n",
       "\n",
       "[39644 rows x 59 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39644 entries, 0 to 39643\n",
      "Data columns (total 59 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   n_tokens_title                 39644 non-null  float64\n",
      " 1   n_tokens_content               39644 non-null  float64\n",
      " 2   n_unique_tokens                39644 non-null  float64\n",
      " 3   n_non_stop_words               39644 non-null  float64\n",
      " 4   n_non_stop_unique_tokens       39644 non-null  float64\n",
      " 5   num_hrefs                      39644 non-null  float64\n",
      " 6   num_self_hrefs                 39644 non-null  float64\n",
      " 7   num_imgs                       39644 non-null  float64\n",
      " 8   num_videos                     39644 non-null  float64\n",
      " 9   average_token_length           39644 non-null  float64\n",
      " 10  num_keywords                   39644 non-null  float64\n",
      " 11  data_channel_is_lifestyle      39644 non-null  float64\n",
      " 12  data_channel_is_entertainment  39644 non-null  float64\n",
      " 13  data_channel_is_bus            39644 non-null  float64\n",
      " 14  data_channel_is_socmed         39644 non-null  float64\n",
      " 15  data_channel_is_tech           39644 non-null  float64\n",
      " 16  data_channel_is_world          39644 non-null  float64\n",
      " 17  kw_min_min                     39644 non-null  float64\n",
      " 18  kw_max_min                     39644 non-null  float64\n",
      " 19  kw_avg_min                     39644 non-null  float64\n",
      " 20  kw_min_max                     39644 non-null  float64\n",
      " 21  kw_max_max                     39644 non-null  float64\n",
      " 22  kw_avg_max                     39644 non-null  float64\n",
      " 23  kw_min_avg                     39644 non-null  float64\n",
      " 24  kw_max_avg                     39644 non-null  float64\n",
      " 25  kw_avg_avg                     39644 non-null  float64\n",
      " 26  self_reference_min_shares      39644 non-null  float64\n",
      " 27  self_reference_max_shares      39644 non-null  float64\n",
      " 28  self_reference_avg_sharess     39644 non-null  float64\n",
      " 29  weekday_is_monday              39644 non-null  float64\n",
      " 30  weekday_is_tuesday             39644 non-null  float64\n",
      " 31  weekday_is_wednesday           39644 non-null  float64\n",
      " 32  weekday_is_thursday            39644 non-null  float64\n",
      " 33  weekday_is_friday              39644 non-null  float64\n",
      " 34  weekday_is_saturday            39644 non-null  float64\n",
      " 35  weekday_is_sunday              39644 non-null  float64\n",
      " 36  is_weekend                     39644 non-null  float64\n",
      " 37  LDA_00                         39644 non-null  float64\n",
      " 38  LDA_01                         39644 non-null  float64\n",
      " 39  LDA_02                         39644 non-null  float64\n",
      " 40  LDA_03                         39644 non-null  float64\n",
      " 41  LDA_04                         39644 non-null  float64\n",
      " 42  global_subjectivity            39644 non-null  float64\n",
      " 43  global_sentiment_polarity      39644 non-null  float64\n",
      " 44  global_rate_positive_words     39644 non-null  float64\n",
      " 45  global_rate_negative_words     39644 non-null  float64\n",
      " 46  rate_positive_words            39644 non-null  float64\n",
      " 47  rate_negative_words            39644 non-null  float64\n",
      " 48  avg_positive_polarity          39644 non-null  float64\n",
      " 49  min_positive_polarity          39644 non-null  float64\n",
      " 50  max_positive_polarity          39644 non-null  float64\n",
      " 51  avg_negative_polarity          39644 non-null  float64\n",
      " 52  min_negative_polarity          39644 non-null  float64\n",
      " 53  max_negative_polarity          39644 non-null  float64\n",
      " 54  title_subjectivity             39644 non-null  float64\n",
      " 55  title_sentiment_polarity       39644 non-null  float64\n",
      " 56  abs_title_subjectivity         39644 non-null  float64\n",
      " 57  abs_title_sentiment_polarity   39644 non-null  float64\n",
      " 58  shares                         39644 non-null  int64  \n",
      "dtypes: float64(58), int64(1)\n",
      "memory usage: 17.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "display(df)\n",
    "display(df.info())\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-PSn5w66Ylo"
   },
   "source": [
    "## 2\n",
    "\n",
    "**q2:** What is the maximum number of shares among all the news articles presented in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tA6IUIgZL0JP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "np.max(df.shares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7SEP8pj9VQf"
   },
   "source": [
    "## 3\n",
    "\n",
    "**q3:** What is the median number of shares for the articles published on Monday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E8acRylcL4eB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "df[df.weekday_is_monday == 1].shares.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJRiUrSt9gWS"
   },
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36BGDOx19peU"
   },
   "source": [
    "First, we separate the target from the dataframe with features (`df` -> `X`, `y`).\n",
    "\n",
    "Next, let's split the data into train/val/test sets in the ratio 60:20:20. The idea is that we will use train set to train our models, val set to validate them and test set to calculate the final error of the blend. So, test set will be a completely unseen data.\n",
    "\n",
    "To do this, use a regular `train_test_split` from `sklearn` to split `X` and `y` into train and val/test parts in the ratio 60:40. Then use `train_test_split` again, but to split the obtain val/test part into validation and test in the ratio 50:50. In each `train_test_split` application, use `random_state=13` and other default parameter values.\n",
    "\n",
    "In the end, you should obtain `X_train`, `X_val`, `X_test` with the following shapes, respectively: (23786, 58), (7929, 58), (7929, 58). The same logic is with `y_train`, `y_val`, `y_test`.\n",
    "\n",
    "**q4:** What is the mean value of target in the test part (`X_test`)? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x18Nt2Ln9hAQ"
   },
   "outputs": [],
   "source": [
    "X = df.drop('shares', axis=1)\n",
    "y = df['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JBpG87eiL7DY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3349.74057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((23786, 58), (7929, 58), (7929, 58), (23786,), (7929,), (7929,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=13)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=13)\n",
    "print(np.round(y_test.mean(), 5))\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zijl0TGp_8P2"
   },
   "source": [
    "## 5\n",
    "\n",
    "Now let's train our first model - XGBoost. A link to the documentation: https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "We will use Scikit-Learn Wrapper interface for XGBoost (and the same logic applies to the following LightGBM and CatBoost models). Here, we work on the regression task - hence we will use `XGBRegressor`. Read about the parameters of `XGBRegressor`: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "\n",
    "The main list of XGBoost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html# Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `XGBRegressor` with MSE objective (`objective='reg:squarederror'`), 200 trees (`n_estimators=200`), `learning_rate=0.01`, `max_depth=5`, `random_state=13` and all other default parameter values. Train it on the train set (`fit` function). \n",
    "\n",
    "**q5:** Calculate Root Mean Squared Error (RMSE) on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bsf1mDiPL-dX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10329.20768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "xgb = XGBRegressor(objective='reg:squarederror', \n",
    "                   n_estimators=200, \n",
    "                   learning_rate=0.01, \n",
    "                   max_depth=5, \n",
    "                   random_state=13)\n",
    "xgb.fit(X_train, y_train)\n",
    "np.round(mean_squared_error(xgb.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9YFI8rhBTmN"
   },
   "source": [
    "## 6\n",
    "\n",
    "In the task 5, we have decided to build 200 trees in our model. However, it is hard to understand whether it is a good decision - maybe it is too much? Maybe 150 is a better number? Or 100? Or 50 is enough?\n",
    "\n",
    "During the training process, it is possible to stop constructing the ensemble if we see that the validation error does not decrease anymore. Using the same XGBoost model, call `fit` function (to train it) with `eval_set=[(X_val, y_val)]` (to evaluate the boosting model after building a new tree) and `early_stopping_rounds=50` (and other default parameter values). This `early_stopping_rounds` says that if the validation metric does not increase on 50 consequent iterations, the training stops.\n",
    "\n",
    "**q6:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "WmX6FqvoMA4L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "xgb = XGBRegressor(objective='reg:squarederror', \n",
    "                   n_estimators=200, \n",
    "                   learning_rate=0.01, \n",
    "                   max_depth=5, \n",
    "                   random_state=13)\n",
    "xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50)\n",
    "np.round(mean_squared_error(xgb.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_jy-BqrCaci"
   },
   "source": [
    "## 7\n",
    "\n",
    "Notes on parameter tuning: https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `XGBRegressor` with the following parameters:\n",
    "\n",
    "* `objective='reg:squarederror'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=4`\n",
    "* `gamma=1`\n",
    "* `subsample=0.5`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 6, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q7:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "E4nDYySWMC_p"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "xgb = XGBRegressor(objective='reg:squarederror',\n",
    "                   n_estimators=5000,\n",
    "                   learning_rate=0.001,\n",
    "                   max_depth=4,\n",
    "                   gamma=1,\n",
    "                   subsample=0.5,\n",
    "                   random_state=13)\n",
    "xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=500)\n",
    "np.round(mean_squared_error(xgb.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57TAxOUfC_-9"
   },
   "source": [
    "## 8\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 7. \n",
    "\n",
    "**q8:** What is the name of the most important feature? Provide it as the answer. Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that by default, `XGBRegressor` calculates feature importance considering gain (`importance_type` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uyWWYYvKMEwO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_channel_is_bus'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "imp = xgb.get_booster().get_score(importance_type='gain')\n",
    "max(imp, key=imp.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4EWKwRkEJGO"
   },
   "source": [
    "## 9\n",
    "\n",
    "Let's move to LightGBM. We will work with `LGBMRegressor`.\n",
    "\n",
    "LGBMRegressor parameters: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor\n",
    "\n",
    "The main list of LightGBM parameters: https://lightgbm.readthedocs.io/en/latest/Parameters.html Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `LGBMRegressor` with the following parameters, similar to the previous `XGBoost` model:\n",
    "\n",
    "* `objective='regression'`\n",
    "* `n_estimators=200`\n",
    "* `learning_rate=0.01`\n",
    "* `max_depth=5`\n",
    "* `random_state=13`\n",
    "* other default parameter values\n",
    "\n",
    "Train it on the training data with `eval_set=[(X_val, y_val)]`, `eval_metric='rmse'`, `early_stopping_rounds=50` and all other default parameter values. \n",
    "\n",
    "**q9:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm and compare it to the speed of XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "4Y-dWz4SMHTB"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "lgbm = LGBMRegressor(objective='regression',\n",
    "                     n_estimators=200,\n",
    "                     learning_rate=0.01,\n",
    "                     max_depth=5,\n",
    "                     random_state=13)\n",
    "lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', early_stopping_rounds=50)\n",
    "np.round(mean_squared_error(lgbm.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9CrcUB3GZg9"
   },
   "source": [
    "## 10\n",
    "\n",
    "Notes on parameter tuning: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `LGBMRegressor` with the following parameters:\n",
    "\n",
    "* `objective='regression'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=3`\n",
    "* `lambda_l2=1.0`\n",
    "* `boosting_type='goss'`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 9, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q10:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ybE86ky2MJLx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matu/.local/lib/python3.8/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/matu/.local/lib/python3.8/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8420.67793"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "lgbm = LGBMRegressor(objective='regression',\n",
    "                     n_estimators=5000,\n",
    "                     learning_rate=0.001,\n",
    "                     max_depth=3,\n",
    "                     lambda_l2=1.0,\n",
    "                     boosting_type='goss',\n",
    "                     importance_type='split',\n",
    "                     random_state=13)\n",
    "lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', early_stopping_rounds=500, verbose=False)\n",
    "np.round(mean_squared_error(lgbm.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRMPMVMqJEJu"
   },
   "source": [
    "## 11\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 10. \n",
    "\n",
    "**q11:** What is the name of the most important feature? Provide it as the answer. \n",
    "\n",
    "Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that by default, `LGBMRegressor` calculates feature importance considering number of times the feature is used in the model (`importance_type` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FYrvkH1rMK6t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value                           1296\n",
       "Feature    self_reference_min_shares\n",
       "Name: 57, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "feature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "feature_imp.iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjzix3YhSy5x"
   },
   "source": [
    "## 12\n",
    "\n",
    "Since some features are not important for the model, we can drop them in order to try to construct a better model which does not consider them at all.\n",
    "\n",
    "Obtain new train and validation sets without the features with LightGBM importance less than 10 (the importances were computed in the task 11). Train the same model as in the task 10 on the new train set in the same manner. \n",
    "\n",
    "**q12:** Calculate RMSE on the new validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice that the new versions of train and validation sets are used only in this task and in blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7FEgaXIXMNkh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matu/.local/lib/python3.8/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/matu/.local/lib/python3.8/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8420.86652"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "cols = list(feature_imp[feature_imp.Value >= 10].Feature)\n",
    "\n",
    "X_train_new = X_train[cols]\n",
    "X_val_new = X_val[cols]\n",
    "lgbm.fit(X_train_new, y_train, eval_set=[(X_val_new, y_val)], eval_metric='rmse', early_stopping_rounds=500, verbose=False)\n",
    "np.round(mean_squared_error(lgbm.predict(X_val_new), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pIu6_n3JvGI"
   },
   "source": [
    "## 13\n",
    "\n",
    "Let's move to CatBoost. We will work with `CatBoostRegressor`.\n",
    "\n",
    "Info about `CatBoostRegressor`: https://catboost.ai/docs/concepts/python-reference_catboostregressor.html\n",
    "\n",
    "CatBoost parameters: https://catboost.ai/docs/concepts/python-reference_parameters-list.html Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `CatBoostRegressor` with the following parameters, similar to the previous models:\n",
    "\n",
    "* `loss_function='RMSE'`\n",
    "* `iterations=200`\n",
    "* `learning_rate=0.01`\n",
    "* `max_depth=5`\n",
    "* `random_state=13`\n",
    "* other default parameter values\n",
    "\n",
    "Train it on the training data with `eval_set=[(X_val, y_val)]`, `early_stopping_rounds=50` and all other default parameter values. \n",
    "\n",
    "**q13:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm and compare it to the speed of XGBoost and LightGBM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "R-rFegFdMPx6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8485.01086"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "catboost = CatBoostRegressor(loss_function='RMSE',\n",
    "                             iterations=200,\n",
    "                             learning_rate=0.01,\n",
    "                             max_depth=5,\n",
    "                             random_state=13)\n",
    "catboost.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "np.round(mean_squared_error(catboost.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keSiZN2DNYbn"
   },
   "source": [
    "## 14\n",
    "\n",
    "Notes on parameter tuning: https://catboost.ai/docs/concepts/parameter-tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `CatBoostRegressor` with the following parameters:\n",
    "\n",
    "* `loss_function='RMSE'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=9`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 13, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q14:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t-1QF1nKMRye"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8465.44037"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "catboost = CatBoostRegressor(loss_function='RMSE',\n",
    "                             iterations=5000,\n",
    "                             learning_rate=0.001,\n",
    "                             max_depth=9,\n",
    "                             random_state=13)\n",
    "catboost.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=500, verbose=False)\n",
    "np.round(mean_squared_error(catboost.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d0kLl7HS3lG"
   },
   "source": [
    "## 15\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 14. \n",
    "\n",
    "**q15:** What is the name of the most important feature? Provide it as the answer. \n",
    "\n",
    "Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that in case of regression, `CatBoostRegressor` calculates feature importance considering PredictionValuesChange: https://catboost.ai/docs/concepts/fstr.html#fstr__regular-feature-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Z0-Bw-IbMVW3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kw_avg_avg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "# catboost.feature_importances_\n",
    "importances = list(catboost.get_feature_importance(type='PredictionValuesChange'))\n",
    "X_train.columns[importances.index(max(importances))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMdjrX7TdSx"
   },
   "source": [
    "## 16\n",
    "\n",
    "Finally, take a `Lasso` model from `sklearn` with `alpha=10.0`, `random_state=13` and all other default parameter values. Train it on the train set. \n",
    "\n",
    "**q16:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "D-3Ie1OmMXbj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8426.97894"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "lasso = Lasso(alpha=10.0, random_state=13)\n",
    "lasso.fit(X_train, y_train)\n",
    "np.round(mean_squared_error(lasso.predict(X_val), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6COwSmbBt1m8"
   },
   "source": [
    "## 17\n",
    "\n",
    "Compare the results on the validation set of the trained models:\n",
    "\n",
    "* XGBoost (task 7)\n",
    "* LightGBM (task 12)\n",
    "* CatBoost (task 14)\n",
    "* Lasso (task 16)\n",
    "\n",
    "**q17:** Which model has the best RMSE value on the validation set? For the answer, provide the following:\n",
    "\n",
    "* 1 (if XGBoost was the best)\n",
    "* 2 (if LightGBM was the best)\n",
    "* 3 (if CatBoost was the best)\n",
    "* 4 (if Lasso was the best)\n",
    "\n",
    "**Answer: 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aEMdDB0cY01"
   },
   "source": [
    "## 18\n",
    "\n",
    "Finally, let's move to blending the models that we obtained. First, calculate the predictions for the trained models on the validation set. Remember that LightGBM model used slightly different set of columns in the data.\n",
    "\n",
    "After getting the predictions for the validation set, concatenate them into a single dataframe `X_val_blend`. The dataframe should look like this:\n",
    "\n",
    "||xgb|lgb|cb|lasso|\n",
    "|-|-|-|-|-|\n",
    "|0|2298.947754|3728.088336|3680.924182|4270.039931|\n",
    "|1|3208.189209|5243.744431|4487.549790|6755.853939|\n",
    "|...|...|...|...|...|\n",
    "\n",
    "Here, `xgb` column represents XGBoost predictions, `lgb` - LightGBM predictions, `cb` - CatBoost predictions, `lasso` - lasso predictions.\n",
    "\n",
    "**q18:** For the answer, calculate the mean value of all model predictions in the last row of this column (`X_val_blend.iloc[-1]`). Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pLLOMwNSUG9k"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "xgb_p, lgb_p, cb_p, lasso_p = xgb.predict(X_val), lgbm.predict(X_val_new), catboost.predict(X_val), lasso.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb</th>\n",
       "      <th>lgb</th>\n",
       "      <th>cb</th>\n",
       "      <th>lasso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2298.947754</td>\n",
       "      <td>3764.246115</td>\n",
       "      <td>3680.924182</td>\n",
       "      <td>4270.039931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3208.189209</td>\n",
       "      <td>5257.539706</td>\n",
       "      <td>4487.549790</td>\n",
       "      <td>6755.853939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1171.030029</td>\n",
       "      <td>2367.345976</td>\n",
       "      <td>2899.190806</td>\n",
       "      <td>960.707930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1715.524292</td>\n",
       "      <td>2996.992798</td>\n",
       "      <td>3102.992450</td>\n",
       "      <td>3280.292136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1780.428223</td>\n",
       "      <td>3073.664996</td>\n",
       "      <td>3404.989586</td>\n",
       "      <td>1586.863807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>2203.913086</td>\n",
       "      <td>3435.528433</td>\n",
       "      <td>3608.619347</td>\n",
       "      <td>4982.351696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7925</th>\n",
       "      <td>2486.425293</td>\n",
       "      <td>3784.573003</td>\n",
       "      <td>3874.454205</td>\n",
       "      <td>4799.275291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>1648.837158</td>\n",
       "      <td>3155.824164</td>\n",
       "      <td>3159.619911</td>\n",
       "      <td>3515.011430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>1368.506592</td>\n",
       "      <td>2544.730617</td>\n",
       "      <td>2971.898824</td>\n",
       "      <td>2246.626186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td>1601.703369</td>\n",
       "      <td>3020.110348</td>\n",
       "      <td>3186.352405</td>\n",
       "      <td>3720.950272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7929 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              xgb          lgb           cb        lasso\n",
       "0     2298.947754  3764.246115  3680.924182  4270.039931\n",
       "1     3208.189209  5257.539706  4487.549790  6755.853939\n",
       "2     1171.030029  2367.345976  2899.190806   960.707930\n",
       "3     1715.524292  2996.992798  3102.992450  3280.292136\n",
       "4     1780.428223  3073.664996  3404.989586  1586.863807\n",
       "...           ...          ...          ...          ...\n",
       "7924  2203.913086  3435.528433  3608.619347  4982.351696\n",
       "7925  2486.425293  3784.573003  3874.454205  4799.275291\n",
       "7926  1648.837158  3155.824164  3159.619911  3515.011430\n",
       "7927  1368.506592  2544.730617  2971.898824  2246.626186\n",
       "7928  1601.703369  3020.110348  3186.352405  3720.950272\n",
       "\n",
       "[7929 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2882.2791"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_blend = pd.DataFrame({'xgb': xgb_p, 'lgb': lgb_p, 'cb': cb_p, 'lasso': lasso_p})\n",
    "display(X_val_blend)\n",
    "np.round(X_val_blend.iloc[-1].mean(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RikEkjthVKX-"
   },
   "source": [
    "## 19\n",
    "\n",
    "Obtain a matrix of pairwise Pearson Correlation Coefficient (PCC) values for the column of the dataframe `X_val_blend`. Find a pair of model predictions with the highest PCC value (don't consider 1.0 values of correlations with themselves). \n",
    "\n",
    "**q19:** What is this value equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "e7wJfNuTMh4A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84445"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "np.round(np.max(np.max(X_val_blend.corr(method='pearson').where(X_val_blend.corr(method='pearson')!=1.0))), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKrXVxTGVaZa"
   },
   "source": [
    "## 20\n",
    "\n",
    "Blend models into the ensemble with the weights 0.25, 0.25, 0.25 and 0.25 (just mean value of the predictions). \n",
    "\n",
    "**q20:** Calculate RMSE of such ensemble on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Compare it with RMSE of each model and think whether this is a good ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NoD2EsNRMjvQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8438.87117"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "np.round(mean_squared_error(X_val_blend.mean(axis=1), y_val)**0.5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBPbgfyadERE"
   },
   "source": [
    "## 21\n",
    "\n",
    "Tune the weights of the ensemble. Run each model weight through `np.linspace(0, 1, 101)`, so that all possible values of each weight will be [0.0, 0.01, 0.02, ..., 0.99, 1.0]. Skip each combinations of weights, if their sum is not equal to 1.0. If the sum of the weights in the combination is equal to 1.0, though, get ensemble prediction on the validation set using these weights and calculate RMSE value.\n",
    "\n",
    "In the end, select a combination of weights with the best RMSE value - these are the best weights for the ensemble. \n",
    "\n",
    "**q21:** What is their corresponding RMSE value equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Compare RMSE value of the ensemble with RMSE values of the models in it. Is the ensemble better?\n",
    "\n",
    "_Hint. You probably want to save RMSE with the corresponding weights for each valid combination into some array. Also this weight tuning might be implemented as quadriple nested loop, or you may think about other ways of implementing it. You can track tuning progress using `tqdm` module._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "H9Tud8e-Mp0C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [06:20<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_RMSE = 8404.703562342625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "X_val_blend_arr = np.array(X_val_blend)\n",
    "best_rmse = 10000\n",
    "for i in tqdm(np.linspace(0, 1, 101)):\n",
    "    for j in np.linspace(0, 1, 101):\n",
    "        for k in np.linspace(0, 1, 101):\n",
    "            for m in np.linspace(0, 1, 101):\n",
    "                if i+j+k+m == 1:\n",
    "                    rmse = mean_squared_error(X_val_blend_arr.dot(np.array([i, j, k, m])), y_val)**0.5\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_weight = [i, j, k, m]\n",
    "print('best_RMSE =', best_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFqDAt-EdKxS"
   },
   "source": [
    "## 22\n",
    "\n",
    "Using the best weights obtained in the task 21, run the best ensemble on the test set. To do this, obtain model predictions on the test set (you can write them to the similar table to the one for the validation set in the task 18). Remember that LightGBM model uses slightly different set of columns.\n",
    "\n",
    "**q22:** Calculate RMSE of the final ensemble on the test set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "dEW1M-s6MtOx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8446.46654"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "X_test_new = X_test[cols]\n",
    "xgb_pt, lgb_pt, cb_pt, lasso_pt = xgb.predict(X_test), lgbm.predict(X_test_new), catboost.predict(X_test), lasso.predict(X_test)\n",
    "X_test_blend = pd.DataFrame({'xgb': xgb_pt, 'lgb': lgb_pt, 'cb': cb_pt, 'lasso': lasso_pt})\n",
    "test_rmse = mean_squared_error(np.array(X_test_blend).dot(np.array(best_weight)), y_test)**0.5\n",
    "np.round(test_rmse, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8xzctGVVD8ZayBsxVM3cd",
   "collapsed_sections": [],
   "name": "Week2_practice.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "ensembling-techniques-task-week-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
